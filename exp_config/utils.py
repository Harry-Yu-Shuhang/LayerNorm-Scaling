import yaml
import os
import torch
import random
import numpy as np
from datasets import load_dataset, DownloadConfig
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer
import os
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
# âœ… æ˜¾å¼ç¦ç”¨ reentrant checkpointing
import torch.utils.checkpoint as checkpoint

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"


class Utils:
    def __init__(self, config_path="exp_config/conf.yaml"):
        # è¯»å– YAML é…ç½®æ–‡ä»¶
        with open(config_path, "r") as f:
            config = yaml.safe_load(f)

        # è§£æä¸åŒç±»åˆ«çš„é…ç½®
        self.experiment_config = config["experiment"]
        self.model_config = config["model"]
        self.training_config = config["training"]
        self.dataset_config = config["dataset"]

        # **å®éªŒå‚æ•°**
        self.seed = self.experiment_config["seed"]
        self.selected_tokens = self.experiment_config["selected_tokens"]

        # **æ¨¡å‹å‚æ•°**
        self.model_names = self.model_config["model_names"]

        # **è®­ç»ƒå‚æ•°**
        self.num_epochs = self.training_config["num_epochs"]
        self.per_device_batch_size = self.training_config["per_device_batch_size"]
        self.output_dir = self.training_config["output_dir"]
        self.logging_steps = self.training_config["logging_steps"]
        self.learning_rate = float(self.training_config["learning_rate"]) 
        self.weight_decay = float(self.training_config["weight_decay"])    
        self.max_grad_norm = float(self.training_config["max_grad_norm"])  
        self.max_steps = int(self.training_config["max_steps"])
        self.warmup_ratio = float(self.training_config["warmup_ratio"])  # âœ… æ·»åŠ  warmup
        self.gradient_accumulation_steps = int(self.training_config["gradient_accumulation_steps"])  # âœ… æ·»åŠ æ¢¯åº¦ç´¯ç§¯


        # **æ•°æ®é›†å‚æ•°**
        self.dataset_variant = self.dataset_config["dataset_variant"]

        # è·å–ç¯å¢ƒå˜é‡ä¸­çš„ DDP å‚æ•°
        self.rank = int(os.environ.get("RANK", 0))  # è¿›ç¨‹ç¼–å·
        self.local_rank = int(os.environ.get("LOCAL_RANK", 0))  # æœ¬åœ° GPU è¿›ç¨‹ç¼–å·
        self.world_size = int(os.environ.get("WORLD_SIZE", 1))  # è¿›ç¨‹æ€»æ•°

        # **åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ**
        if self.world_size > 1:
            dist.init_process_group(
                backend="nccl",
                init_method="env://",
                world_size=self.world_size,
                rank=self.rank
            )
            torch.cuda.set_device(self.local_rank)  # ç»‘å®šå½“å‰è¿›ç¨‹åˆ° GPU
            print(f"âœ… è¿›ç¨‹ {self.rank}/{self.world_size} ç»‘å®šåˆ° GPU {self.local_rank}")

        # **è®¾ç½®è®¾å¤‡**
        self.device = torch.device(f"cuda:{self.local_rank}" if torch.cuda.is_available() else "cpu")

        # è®¾ç½®éšæœºç§å­
        self.set_seed(self.seed)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)

        # æ”¾åœ¨ __init__ ä¸­
        if hasattr(checkpoint, "_set_checkpoint_engine"):
            try:
                checkpoint._set_checkpoint_engine("non_reentrant")
                print("âœ… checkpoint å¼•æ“è®¾ç½®ä¸º non_reentrant")
            except Exception as e:
                print(f"âš ï¸ è®¾ç½® checkpoint å¼•æ“å¤±è´¥: {e}")


    def set_seed(self, seed):
        """è®¾ç½®éšæœºç§å­"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(seed)

    def load_c4_data(self):
        """åŠ è½½ C4 æ•°æ®é›†"""
        print(f"ğŸ”µ åŠ è½½ C4 æ•°æ®é›† ({self.dataset_variant})")
        download_config = DownloadConfig(
            resume_download=True,
            max_retries=6,  # å¦‚æœè¶…æ—¶ï¼Œæœ€å¤šé‡è¯• 6 æ¬¡
            storage_options={"timeout": 600}  # è®¾ç½® 10 åˆ†é’Ÿè¶…æ—¶ï¼Œé¿å…å¤ªå¿«æ–­å¼€
        )

        dataset = load_dataset(
            "allenai/c4", 
            self.dataset_variant, 
            split="train", 
            streaming=True,
            data_files="en/c4-train.0000*-of-01024.json.gz",
            download_config=download_config,
        )

        return dataset


    def load_tokenizer_and_model(self, model_name):
        """åŠ è½½ tokenizer å’Œ Transformer æ¨¡å‹ï¼ˆé€‚é… checkpoint & DDPï¼‰"""
        print(f"ğŸ”µ æ­£åœ¨åŠ è½½æ¨¡å‹ {model_name} åˆ° {self.device}...")

        # âœ… è®¾ç½®å…¨å±€ checkpoint å¼•æ“ï¼ˆå¿…é¡»åœ¨æ¨¡å‹åŠ è½½å‰è®¾ç½®ï¼‰
        if hasattr(checkpoint, "_set_checkpoint_engine"):
            try:
                checkpoint._set_checkpoint_engine("non_reentrant")
                print("âœ… è®¾ç½® torch.utils.checkpoint å¼•æ“ä¸º non_reentrant")
            except Exception as e:
                print(f"âš ï¸ è®¾ç½® checkpoint å¼•æ“å¤±è´¥: {e}")
        else:
            print("âš ï¸ å½“å‰ PyTorch ç‰ˆæœ¬ä¸æ”¯æŒ _set_checkpoint_engine")

        # âœ… åŠ è½½ tokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token  # DeepSeek å…¼å®¹æ€§ fix

        # âœ… åŠ è½½æ¨¡å‹
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map=None  # è®© .to(self.device) ç”Ÿæ•ˆ
        ).to(self.device)

        # âœ… è®¾ç½®é…ç½®é¡¹
        model.config.use_cache = False
        try:
            model.gradient_checkpointing_enable(use_reentrant=False)
            print("âœ… å¯ç”¨ gradient checkpointingï¼Œä¸”ç¦ç”¨ reentrant")
        except TypeError:
            model.gradient_checkpointing_enable()
            print("âš ï¸ å½“å‰ transformers ä¸æ”¯æŒ use_reentrant å‚æ•°ï¼Œå¯ç”¨é»˜è®¤ checkpointing")

        print(f"âœ… use_cache: {model.config.use_cache}")
        print(f"âœ… gradient checkpointing: {model.is_gradient_checkpointing}")

        # âœ… é€‚é… DDP å¤šå¡è®­ç»ƒ
        if torch.cuda.device_count() > 1 and torch.distributed.is_initialized():
            model = DDP(
                model,
                device_ids=[self.local_rank],
                output_device=self.local_rank,
                find_unused_parameters=False
            )
            print(f"âœ… æ¨¡å‹ä½¿ç”¨ DDP åˆ†å¸ƒå¼åŒ…è£¹ (GPU {self.local_rank})")

        print(f"âœ… {model_name} åŠ è½½å®Œæˆï¼")
        return tokenizer, model

    def get_training_args(self):
        """è¿”å› Hugging Face Trainer çš„è®­ç»ƒå‚æ•°"""
        return TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=self.num_epochs,
            per_device_train_batch_size=self.per_device_batch_size,
            learning_rate=self.learning_rate,  
            weight_decay=self.weight_decay,
            max_grad_norm=self.max_grad_norm,
            warmup_ratio=self.warmup_ratio,  # âœ… æ·»åŠ  warmup
            gradient_accumulation_steps=self.gradient_accumulation_steps,  # âœ… æ·»åŠ æ¢¯åº¦ç´¯ç§¯
            max_steps=self.max_steps,
            logging_dir=f"{self.output_dir}/logs",
            logging_steps=self.logging_steps,
            save_total_limit=2,
            report_to="none",
            gradient_checkpointing=False,  # âœ… æ˜¾å¼ç¦ç”¨ï¼Œé˜²æ­¢è¦†ç›– model.gradient_checkpointing_enable()
            bf16=torch.cuda.is_bf16_supported(),  # å…¼å®¹æ€§æ›´å¼º
            fp16=not torch.cuda.is_bf16_supported(),
            optim="adamw_torch",  # âœ… ä½¿ç”¨ `adamw_torch` å…¼å®¹ `accelerate`
            ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,  # âœ… é¿å… DDP è®¾å¤‡é—®é¢˜
            dataloader_pin_memory=False,  # âœ… é¿å…å†…å­˜æ³„æ¼
            torch_compile=False,  # âœ… é¿å… `torch.compile` å¼•å‘çš„è®¾å¤‡å†²çª
            ddp_backend="nccl" if self.world_size > 1 else None,  # âœ… é€‚é… DDP
        )