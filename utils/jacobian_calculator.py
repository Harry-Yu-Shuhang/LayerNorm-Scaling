import os
import numpy as np
import torch
from tqdm import tqdm
import pickle
from peft_pretraining.modeling_llama import LlamaRMSNorm  # æ ¹æ®ä½ é¡¹ç›®è·¯å¾„æ›¿æ¢

class JacobianCalculator:
    def __init__(self, output_dir="results/Jacobian"):
        self.output_dir = output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.error_log_path = os.path.join(self.output_dir, "error.log")

        with open(self.error_log_path, "w") as f:
            f.write("")

    def _log_error(self, msg):
        with open(self.error_log_path, "a") as f:
            f.write(msg + "\n")
        print(f"ğŸ›‘ {msg}")

    def compute_jacobian(self, model, model_name, step, input_ids, attention_mask):
        if torch.distributed.is_initialized() and torch.distributed.get_rank() != 0:
            print(f"ğŸ”• Rank {torch.distributed.get_rank()} ä¸ä¿å­˜ Jacobianï¼Œè·³è¿‡ã€‚")
            return {}, {}

        device = next(model.parameters()).device
        print(f"\nğŸŸ¢ Step {step} - åœ¨ {device} ä¸Šè®¡ç®— Jacobian")
        print(f"ğŸ” æ³¨å†Œ LayerNorm (RMSNorm) Hook")

        input_ids = input_ids.to(device)
        attention_mask = attention_mask.to(device)

        input_embeddings = model.get_input_embeddings()(input_ids)
        input_embeddings.requires_grad_()

        # æ³¨å†Œ forward hook æ•è· RMSNorm è¾“å…¥
        norm_inputs = {}

        def make_hook_fn(layer_index, tag):
            def hook_fn(module, input, output):
                key = f"layer_{layer_index}_{tag}"
                try:
                    norm_inputs[key] = input[0].detach().clone().requires_grad_()
                    print(f"âœ… Hook æˆåŠŸ: {key} -> {norm_inputs[key].shape}, requires_grad={norm_inputs[key].requires_grad}")
                except Exception as e:
                    self._log_error(f"Hook æ•è·å¤±è´¥: {key} - é”™è¯¯ä¿¡æ¯: {e}")
            return hook_fn

        handles = []
        for i, layer in enumerate(model.model.layers):
            if hasattr(layer, 'input_layernorm'):
                handles.append(layer.input_layernorm.register_forward_hook(make_hook_fn(i, "input")))
            if hasattr(layer, 'post_attention_layernorm'):
                handles.append(layer.post_attention_layernorm.register_forward_hook(make_hook_fn(i, "post")))

        with torch.no_grad():  # æ¨ç†ä¸éœ€è¦åå‘å›¾
            outputs = model(
                inputs_embeds=input_embeddings,
                attention_mask=attention_mask,
                return_dict=True,
                output_hidden_states=True
            )

        for handle in handles:
            handle.remove()

        hidden_states = outputs.hidden_states
        num_layers = len(hidden_states) - 1
        hidden_dim = hidden_states[1].shape[-1]
        seq_len = attention_mask[0].sum().item()
        selected_tokens = list(range(seq_len))

        print(f"ğŸ“ é pad token æ•°: {seq_len}, éšå±‚ç»´åº¦: {hidden_dim}, å±‚æ•°: {num_layers}")
        jacobian_results = {}
        frobenius_results = {}
        mse_results = {}

        for layer in tqdm(range(num_layers), desc=f"Step {step} - Jacobian", unit="layer"):
            ln_key = f"layer_{layer}_input"
            if ln_key not in norm_inputs:
                self._log_error(f"â›”ï¸ æ²¡æ‰¾åˆ° Layer {layer} çš„ RMSNorm è¾“å…¥ ({ln_key})ï¼Œè·³è¿‡")
                continue

            ln_output = norm_inputs[ln_key]
            if not ln_output.requires_grad:
                self._log_error(f"âš ï¸ Layer {layer} çš„ RMSNorm è¾“å…¥ä¸æ”¯æŒæ¢¯åº¦è®¡ç®—ï¼Œè·³è¿‡")
                continue

            print(f"ğŸ“˜ æ­£åœ¨å¤„ç† Layer {layer}: norm_input shape: {ln_output.shape}")
            layer_jacobians = {"attention": {}, "ffn": {}}
            frob_layer = {"attention": {}, "ffn": {}}
            mse_layer = {"attention": {}, "ffn": {}}

            for token_idx in selected_tokens:
                try:
                    attn_output = hidden_states[layer + 1][0, token_idx, :]
                    print(f"ğŸ” Layer {layer}, Token {token_idx}, Attention è¾“å‡º shape: {attn_output.shape}")
                    jacobian_attn = self._compute_single_jacobian(attn_output, ln_output, token_idx, layer, "attention")
                    if jacobian_attn is not None:
                        layer_jacobians["attention"][token_idx] = jacobian_attn
                        frob_layer["attention"][token_idx] = np.linalg.norm(jacobian_attn, ord="fro")
                        mse_layer["attention"][token_idx] = np.mean(jacobian_attn ** 2)

                    # FFN è¾“å‡ºä¹Ÿç”¨ layer+1ï¼ˆå¯æ ¹æ®æ¨¡å‹ç»“æ„çµæ´»è°ƒæ•´ï¼‰
                    ffn_output = hidden_states[layer + 1][0, token_idx, :]
                    print(f"ğŸ” Layer {layer}, Token {token_idx}, FFN è¾“å‡º shape: {ffn_output.shape}")
                    jacobian_ffn = self._compute_single_jacobian(ffn_output, ln_output, token_idx, layer, "ffn")
                    if jacobian_ffn is not None:
                        layer_jacobians["ffn"][token_idx] = jacobian_ffn
                        frob_layer["ffn"][token_idx] = np.linalg.norm(jacobian_ffn, ord="fro")
                        mse_layer["ffn"][token_idx] = np.mean(jacobian_ffn ** 2)

                except Exception as e:
                    self._log_error(f"âŒ Layer {layer}, Token {token_idx} é”™è¯¯: {e}")

            if layer_jacobians["attention"] or layer_jacobians["ffn"]:
                jacobian_results[layer] = layer_jacobians
                frobenius_results[layer] = frob_layer
                mse_results[layer] = mse_layer

        with open(self.error_log_path, "r") as f:
            error_lines = f.readlines()

        if jacobian_results and len(error_lines) == 0:
            save_path = os.path.join(self.output_dir, f"{model_name}_step_{step}_jacobian.pkl")
            with open(save_path, "wb") as f:
                pickle.dump({
                    "jacobian": jacobian_results,
                    "frobenius": frobenius_results,
                    "mse": mse_results,
                }, f)
            print(f"âœ… Jacobian å·²ä¿å­˜è‡³ {save_path}")
            return frobenius_results, mse_results
        else:
            print("ğŸš« æ£€æµ‹åˆ°é”™è¯¯æˆ–ç»“æœä¸ºç©ºï¼Œæœªä¿å­˜")
            return {}, {}

    def _compute_single_jacobian(self, token_output, ln_output, token_idx, layer_idx, tag):
        jacobian = []
        hidden_dim = token_output.shape[-1]
        for dim in range(hidden_dim):
            grad_outputs = torch.zeros_like(token_output)
            grad_outputs[dim] = 1.0
            try:
                grads = torch.autograd.grad(
                    outputs=token_output,
                    inputs=ln_output,
                    grad_outputs=grad_outputs,
                    retain_graph=True,
                    allow_unused=True,
                )[0]
            except Exception as e:
                self._log_error(f"ğŸ§¨ grad å‡ºé”™ - Layer {layer_idx}, Token {token_idx}, Dim {dim} ({tag}) - {e}")
                return None

            if grads is None:
                self._log_error(f"ğŸš« Grad ä¸º None - Layer {layer_idx}, Token {token_idx}, Dim {dim} ({tag})")
                return None

            grads = grads[0, token_idx, :]
            grads = torch.nan_to_num(grads, nan=0.0, posinf=1.0, neginf=-1.0)
            jacobian.append(grads.detach().cpu().numpy())

        return np.stack(jacobian, axis=0)  # shape: (output_dim, input_dim)
