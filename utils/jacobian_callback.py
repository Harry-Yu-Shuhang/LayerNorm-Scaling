from transformers import TrainerCallback

class JacobianCallback(TrainerCallback):
    """åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è®¡ç®— Jacobian"""
    def __init__(self, jacobian_calculator, model_name, tokenizer):
        super().__init__()
        self.jacobian_calculator = jacobian_calculator
        self.model_name = model_name
        self.tokenizer = tokenizer
        self.trainer = None

    def set_trainer(self, trainer):
        self.trainer = trainer

    def on_epoch_end(self, args, state, control, **kwargs):
        if self.trainer is None:
            print("âŒ Trainer æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ Jacobian è®¡ç®—")
            return

        print(f"ğŸŸ¢ Epoch {state.epoch:.0f} ç»“æŸï¼Œå¼€å§‹è®¡ç®— Jacobian...")

        dataset = self.trainer.train_dataset
        sample = next(iter(dataset))

        input_ids = sample["input_ids"].unsqueeze(0)  # åŠ  batch ç»´åº¦
        attention_mask = sample["attention_mask"].unsqueeze(0)

        decoded_text = self.tokenizer.decode(
            input_ids[0, attention_mask[0].bool()],
            skip_special_tokens=True
        )
        print(f"ğŸ“Œ Jacobian è®¡ç®—æ–‡æœ¬: {decoded_text}")
        print(f"ğŸ“Œ input_ids.shape: {input_ids.shape}, mask.shape: {attention_mask.shape}")

        self.jacobian_calculator.compute_jacobian(
            model=self.trainer.model,
            model_name=self.model_name,
            step=int(state.epoch),
            input_ids=input_ids,
            attention_mask=attention_mask
        )
