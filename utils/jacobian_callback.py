#å¾®è°ƒhuggingfaceéœ€è¦
# from transformers import TrainerCallback

# class JacobianCallback(TrainerCallback):
#     """åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è®¡ç®— Jacobian"""
#     def __init__(self, jacobian_calculator, model_name, tokenizer, max_tokens=256):
#         super().__init__()
#         self.jacobian_calculator = jacobian_calculator
#         self.model_name = model_name
#         self.tokenizer = tokenizer
#         self.trainer = None
#         self.max_tokens = max_tokens

#     def set_trainer(self, trainer):
#         self.trainer = trainer

#     def on_epoch_end(self, args, state, control, **kwargs):
#         if self.trainer is None:
#             print("âŒ Trainer æœªåˆå§‹åŒ–ï¼Œè·³è¿‡ Jacobian è®¡ç®—")
#             return

#         print(f"ðŸŸ¢ Epoch {state.epoch:.0f} ç»“æŸï¼Œå¼€å§‹è®¡ç®— Jacobian...")

#         # èŽ·å–æ ·æœ¬ï¼ˆåªå–ç¬¬ä¸€ä¸ªï¼Œé¿å…æ‰¹é‡è¿‡å¤§ï¼‰
#         dataset = self.trainer.train_dataset
#         sample = next(iter(dataset))

#         input_ids = sample["input_ids"].unsqueeze(0)
#         attention_mask = sample["attention_mask"].unsqueeze(0)

#         # é˜²æ­¢å¤ªé•¿å¯¼è‡´è®¡ç®—çˆ†ç‚¸
#         if input_ids.shape[1] > self.max_tokens:
#             input_ids = input_ids[:, :self.max_tokens]
#             attention_mask = attention_mask[:, :self.max_tokens]

#         decoded_text = self.tokenizer.decode(
#             input_ids[0, attention_mask[0].bool()],
#             skip_special_tokens=True
#         )
#         print(f"ðŸ“Œ Jacobian è®¡ç®—æ–‡æœ¬: {decoded_text}")
#         print(f"ðŸ“Œ input_ids.shape: {input_ids.shape}, mask.shape: {attention_mask.shape}")

#         self.jacobian_calculator.compute_jacobian(
#             model=self.trainer.model,
#             model_name=self.model_name,
#             step=int(state.epoch),
#             input_ids=input_ids,
#             attention_mask=attention_mask
#         )
