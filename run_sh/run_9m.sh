#!/bin/bash

# ä»å‚æ•°ä¸­è¯»å–
norm_type=$1
post_num=$2

export NORM_TYPE=$norm_type
export POST_NUM=$post_num

# ä» conf.yaml ä¸­è¯»å– learning_rate
learning_rate=$(python -c "import yaml; print(yaml.safe_load(open('exp_config/conf.yaml'))['training']['learning_rate'])")

# è‡ªåŠ¨è·å–åˆ†é…åˆ°çš„ GPU æ•°é‡ï¼ˆé»˜è®¤ä¸º1ï¼‰
NUM_GPUS=${SLURM_GPUS_ON_NODE:-1}

# é»˜è®¤ä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU
CUDA_VISIBLE_DEVICES=$(seq -s, 0 $((NUM_GPUS - 1)))

echo "ğŸš€ å¼€å§‹è®­ç»ƒ 9M æ¨¡å‹ | Norm: $norm_type | Post: $post_num | LR: $learning_rate | GPUs: $NUM_GPUS"
echo "ğŸ”§ ä½¿ç”¨çš„ GPU: $CUDA_VISIBLE_DEVICES"

# å¯åŠ¨è®­ç»ƒä»»åŠ¡
CUDA_VISIBLE_DEVICES=$CUDA_VISIBLE_DEVICES torchrun --nproc_per_node=$NUM_GPUS --master_port=29511 main_new.py \
    --model_config configs/llama_9m.json \
    --lr $learning_rate \
    --batch_size 4 \
    --total_batch_size 32 \
    --num_training_steps 10000 \
    --warmup_steps 500 \
    --weight_decay 0 \
    --dtype bfloat16 \
    --eval_every 1000 \
    --optimizer adam \
    --grad_clipping 0.0 \
    --run_name "9m_res_${norm_type}_lr${learning_rate}" \
    --save_dir "9m_res_${norm_type}_lr${learning_rate}"
